{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1102cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:01.906418Z",
     "iopub.status.busy": "2022-02-23T05:09:01.905463Z",
     "iopub.status.idle": "2022-02-23T05:09:06.476687Z",
     "shell.execute_reply": "2022-02-23T05:09:06.477439Z",
     "shell.execute_reply.started": "2022-02-23T04:59:51.916818Z"
    },
    "papermill": {
     "duration": 4.663889,
     "end_time": "2022-02-23T05:09:06.477648",
     "exception": false,
     "start_time": "2022-02-23T05:09:01.813759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ada57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:06.853204Z",
     "iopub.status.busy": "2022-02-23T05:09:06.852356Z",
     "iopub.status.idle": "2022-02-23T05:09:06.855417Z",
     "shell.execute_reply": "2022-02-23T05:09:06.855011Z",
     "shell.execute_reply.started": "2022-02-23T04:59:56.735856Z"
    },
    "papermill": {
     "duration": 0.135189,
     "end_time": "2022-02-23T05:09:06.855536",
     "exception": false,
     "start_time": "2022-02-23T05:09:06.720347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"seed\": 42, \n",
    "          \"img_size\": 768,\n",
    "          \"model_name\": \"tf_efficientnet_b6_ns\", # tf_efficientnet_b6_ns, tf_efficientnetv2_l_in21k, eca_nfnet_l2 \n",
    "          \"num_classes\": 15587, \n",
    "          \"embedding_size\": 512, \n",
    "          \"train_batch_size\": 64, \n",
    "          \"valid_batch_size\": 64, \n",
    "          \"n_fold\": 5, \n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          \"gpu_parallel\":True, \n",
    "          \"image_data\":\"fullbody\", \n",
    "          \"debug\":True, \n",
    "          \"num_workers\":10, \n",
    "          \"s\": 30.0, \n",
    "          \"m\": 0.30, \n",
    "          \"ls_eps\": 0.0, \n",
    "          \"easy_margin\": False, \n",
    "          \"KNN\":850,\n",
    "          \n",
    "          }\n",
    "\n",
    "if CONFIG[\"debug\"]:\n",
    "    CONFIG[\"img_size\"] = 512\n",
    "    CONFIG[\"model_name\"] = \"tf_efficientnet_b0_ns\"\n",
    "    CONFIG[\"train_batch_size\"] = 32\n",
    "    CONFIG[\"valid_batch_size\"] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cfc6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:07.190017Z",
     "iopub.status.busy": "2022-02-23T05:09:07.189339Z",
     "iopub.status.idle": "2022-02-23T05:09:07.197816Z",
     "shell.execute_reply": "2022-02-23T05:09:07.197367Z",
     "shell.execute_reply.started": "2022-02-23T04:59:56.790906Z"
    },
    "papermill": {
     "duration": 0.095411,
     "end_time": "2022-02-23T05:09:07.197943",
     "exception": false,
     "start_time": "2022-02-23T05:09:07.102532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # set True to be faster\n",
    "seed_everything(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e57e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:07.364310Z",
     "iopub.status.busy": "2022-02-23T05:09:07.363433Z",
     "iopub.status.idle": "2022-02-23T05:09:07.365261Z",
     "shell.execute_reply": "2022-02-23T05:09:07.365693Z",
     "shell.execute_reply.started": "2022-02-23T04:59:56.806296Z"
    },
    "papermill": {
     "duration": 0.087356,
     "end_time": "2022-02-23T05:09:07.365844",
     "exception": false,
     "start_time": "2022-02-23T05:09:07.278488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/workspace'\n",
    "ROOT_DIR = '/home/workspace/happy-whale-and-dolphin'\n",
    "\n",
    "if CONFIG[\"image_data\"] == \"backfins\":\n",
    "    TRAIN_DIR = f'{ROOT_DIR}/train_backfins_images'\n",
    "    TEST_DIR = f'{ROOT_DIR}/test_backfins_images'\n",
    "    \n",
    "elif CONFIG[\"image_data\"] == \"fullbody\":\n",
    "    TRAIN_DIR = f'{ROOT_DIR}/train_fullbody_images'\n",
    "    TEST_DIR = f'{ROOT_DIR}/test_fullbody_images'\n",
    "    \n",
    "else:\n",
    "    TRAIN_DIR = f'{ROOT_DIR}/train_images'\n",
    "    TEST_DIR = f'{ROOT_DIR}/test_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce397f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:07.859225Z",
     "iopub.status.busy": "2022-02-23T05:09:07.858351Z",
     "iopub.status.idle": "2022-02-23T05:09:07.985429Z",
     "shell.execute_reply": "2022-02-23T05:09:07.985895Z",
     "shell.execute_reply.started": "2022-02-23T04:59:56.824790Z"
    },
    "papermill": {
     "duration": 0.212231,
     "end_time": "2022-02-23T05:09:07.986051",
     "exception": false,
     "start_time": "2022-02-23T05:09:07.773820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_file_path(id):\n",
    "    return f\"{TRAIN_DIR}/{id}\"\n",
    "\n",
    "df = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\n",
    "df['file_path'] = df['image'].apply(get_train_file_path) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583ffdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:08.165948Z",
     "iopub.status.busy": "2022-02-23T05:09:08.165175Z",
     "iopub.status.idle": "2022-02-23T05:09:08.201980Z",
     "shell.execute_reply": "2022-02-23T05:09:08.201445Z",
     "shell.execute_reply.started": "2022-02-23T04:59:56.939790Z"
    },
    "papermill": {
     "duration": 0.135918,
     "end_time": "2022-02-23T05:09:08.202120",
     "exception": false,
     "start_time": "2022-02-23T05:09:08.066202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "with open(f'{ROOT_DIR}/le.pkl', \"rb\") as fp:\n",
    "    encoder = joblib.load(fp)\n",
    "    \n",
    "df['individual_id'] = encoder.transform(df['individual_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544990c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:08.370561Z",
     "iopub.status.busy": "2022-02-23T05:09:08.369695Z",
     "iopub.status.idle": "2022-02-23T05:09:08.837347Z",
     "shell.execute_reply": "2022-02-23T05:09:08.836746Z",
     "shell.execute_reply.started": "2022-02-23T04:59:56.973632Z"
    },
    "papermill": {
     "duration": 0.554876,
     "end_time": "2022-02-23T05:09:08.837477",
     "exception": false,
     "start_time": "2022-02-23T05:09:08.282601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CONFIG['n_fold'])\n",
    "for fold, ( _, val_) in enumerate(skf.split(X=df, y=df.individual_id)):\n",
    "      df.loc[val_ , \"kfold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516bd05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:09.172469Z",
     "iopub.status.busy": "2022-02-23T05:09:09.170894Z",
     "iopub.status.idle": "2022-02-23T05:09:09.173063Z",
     "shell.execute_reply": "2022-02-23T05:09:09.173464Z",
     "shell.execute_reply.started": "2022-02-23T04:59:57.432756Z"
    },
    "papermill": {
     "duration": 0.091753,
     "end_time": "2022-02-23T05:09:09.173623",
     "exception": false,
     "start_time": "2022-02-23T05:09:09.081870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HappyWhaleDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df \n",
    "        self.ids = df['image'].values \n",
    "        self.file_names = df['file_path'].values \n",
    "        self.labels = df['individual_id'].values \n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        idx = self.ids[index]  \n",
    "        img_path = self.file_names[index] \n",
    "        img = cv2.imread(img_path) \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"] \n",
    "            \n",
    "        return {\n",
    "            'image': img, \n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'id': idx \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab578f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:09.505849Z",
     "iopub.status.busy": "2022-02-23T05:09:09.504905Z",
     "iopub.status.idle": "2022-02-23T05:09:09.506666Z",
     "shell.execute_reply": "2022-02-23T05:09:09.507167Z",
     "shell.execute_reply.started": "2022-02-23T04:59:57.442363Z"
    },
    "papermill": {
     "duration": 0.090063,
     "end_time": "2022-02-23T05:09:09.507329",
     "exception": false,
     "start_time": "2022-02-23T05:09:09.417266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据增强\n",
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']), \n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.),\n",
    "    \n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p) \n",
    "        self.eps = eps  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23165d1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:10.179824Z",
     "iopub.status.busy": "2022-02-23T05:09:10.178836Z",
     "iopub.status.idle": "2022-02-23T05:09:10.181012Z",
     "shell.execute_reply": "2022-02-23T05:09:10.181388Z",
     "shell.execute_reply.started": "2022-02-23T04:59:57.466699Z"
    },
    "papermill": {
     "duration": 0.096308,
     "end_time": "2022-02-23T05:09:10.181535",
     "exception": false,
     "start_time": "2022-02-23T05:09:10.085227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Arcface\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, \n",
    "                 m=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features \n",
    "        self.out_features = out_features \n",
    "        self.s = s \n",
    "        self.m = m \n",
    "        self.ls_eps = ls_eps  \n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin \n",
    "        self.cos_m = math.cos(m) # cos margin\n",
    "        self.sin_m = math.sin(m) # sin margin\n",
    "        self.threshold = math.cos(math.pi - m) # cos(pi - m) = -cos(m)\n",
    "        self.mm = math.sin(math.pi - m) * m # sin(pi - m)*m = sin(m)*m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight)) \n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2)) \n",
    "        phi = cosine * self.cos_m - sine * self.sin_m # cosθ*cosm – sinθ*sinm = cos(θ + m)\n",
    "        phi = phi.float() # phi to float\n",
    "        cosine = cosine.float() # cosine to float\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            # if cos(θ) > cos(pi - m) means θ + m < math.pi, so phi = cos(θ + m);\n",
    "            # else means θ + m >= math.pi, we use Talyer extension to approximate the cos(θ + m).\n",
    "            # if fact, cos(θ + m) = cos(θ) - m * sin(θ) >= cos(θ) - m * sin(math.pi - m)\n",
    "            phi = torch.where(cosine > self.threshold, phi, cosine - self.mm) \n",
    "        # https://github.com/ronghuaiyang/arcface-pytorch/issues/48\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device=CONFIG['device'])\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # label smoothing\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine) \n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0227c63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:10.515682Z",
     "iopub.status.busy": "2022-02-23T05:09:10.515050Z",
     "iopub.status.idle": "2022-02-23T05:09:16.208014Z",
     "shell.execute_reply": "2022-02-23T05:09:16.207079Z",
     "shell.execute_reply.started": "2022-02-23T04:59:57.485440Z"
    },
    "papermill": {
     "duration": 5.785999,
     "end_time": "2022-02-23T05:09:16.208162",
     "exception": false,
     "start_time": "2022-02-23T05:09:10.422163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HappyWhaleModel(nn.Module):\n",
    "    def __init__(self, model_name, embedding_size, pretrained=True):\n",
    "        super(HappyWhaleModel, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'efficientnet' in model_name:\n",
    "            in_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Identity()\n",
    "            self.model.global_pool = nn.Identity()\n",
    "        elif 'nfnet' in model_name:\n",
    "            in_features = self.model.head.fc.in_features\n",
    "            self.model.head.fc = nn.Identity()\n",
    "            self.model.head.global_pool = nn.Identity()\n",
    "\n",
    "        self.pooling = GeM() \n",
    "        self.embedding = nn.Sequential(\n",
    "                            nn.BatchNorm1d(in_features),\n",
    "                            nn.Linear(in_features, embedding_size)\n",
    "                            )\n",
    "        # arcface\n",
    "        self.fc = ArcMarginProduct(embedding_size, \n",
    "                                   CONFIG[\"num_classes\"], \n",
    "                                   s=CONFIG[\"s\"],\n",
    "                                   m=CONFIG[\"m\"], \n",
    "                                   easy_margin=CONFIG[\"easy_margin\"], \n",
    "                                   ls_eps=CONFIG[\"ls_eps\"]) \n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        features = self.model(images) \n",
    "        pooled_features = self.pooling(features).flatten(1) \n",
    "        embedding = self.embedding(pooled_features) # embedding\n",
    "        output = self.fc(embedding, labels) # arcface\n",
    "        return output\n",
    "    \n",
    "    def extract(self, images):\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1) # gem pooling\n",
    "        embedding = self.embedding(pooled_features) # embedding\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf96a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:16.382897Z",
     "iopub.status.busy": "2022-02-23T05:09:16.382016Z",
     "iopub.status.idle": "2022-02-23T05:09:16.384336Z",
     "shell.execute_reply": "2022-02-23T05:09:16.384757Z",
     "shell.execute_reply.started": "2022-02-23T05:00:02.291683Z"
    },
    "papermill": {
     "duration": 0.095398,
     "end_time": "2022-02-23T05:09:16.384911",
     "exception": false,
     "start_time": "2022-02-23T05:09:16.289513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_embeddings(model, dataloader, device):\n",
    "    model.to(CONFIG['device'])\n",
    "    model.eval()\n",
    "    \n",
    "    LABELS = []\n",
    "    EMBEDS = []\n",
    "    IDS = []\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader)) \n",
    "    for step, data in bar:        \n",
    "        images = data['image'].to(device, dtype=torch.float) \n",
    "        labels = data['label'].to(device, dtype=torch.long) \n",
    "        ids = data['id'] \n",
    "\n",
    "        outputs = model.extract(images)\n",
    "        \n",
    "        LABELS.append(labels.cpu().numpy()) \n",
    "        EMBEDS.append(outputs.cpu().numpy()) \n",
    "        IDS.append(ids) \n",
    "    \n",
    "    EMBEDS = np.vstack(EMBEDS) \n",
    "    LABELS = np.concatenate(LABELS) \n",
    "    IDS = np.concatenate(IDS) \n",
    "    \n",
    "    return EMBEDS, LABELS, IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1103ac6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T05:09:16.570050Z",
     "iopub.status.busy": "2022-02-23T05:09:16.569386Z",
     "iopub.status.idle": "2022-02-23T05:09:16.572414Z",
     "shell.execute_reply": "2022-02-23T05:09:16.572850Z",
     "shell.execute_reply.started": "2022-02-23T05:00:02.302901Z"
    },
    "papermill": {
     "duration": 0.103684,
     "end_time": "2022-02-23T05:09:16.573006",
     "exception": false,
     "start_time": "2022-02-23T05:09:16.469322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_loaders(df, fold):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    "    \n",
    "    train_dataset = HappyWhaleDataset(df_train, transforms=data_transforms[\"train\"])\n",
    "    valid_dataset = HappyWhaleDataset(df_valid, transforms=data_transforms[\"valid\"]) \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], num_workers=CONFIG[\"num_workers\"], shuffle=False, pin_memory=False) # Train DataLoader\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=CONFIG[\"num_workers\"], shuffle=False, pin_memory=False) # Valid DataLoader\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ccfe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df[\"image\"] = os.listdir(f\"{ROOT_DIR}/test_images\")\n",
    "test_df[\"file_path\"] = test_df[\"image\"].apply(lambda x: f\"{TEST_DIR}/{x}\")\n",
    "test_df[\"individual_id\"] = -1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d296252",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = prepare_loaders(df, fold=0) \n",
    "test_dataset = HappyWhaleDataset(test_df, transforms=data_transforms[\"valid\"]) \n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=CONFIG[\"num_workers\"], shuffle=False, pin_memory=False) # test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_dict = {\n",
    "    \"weights_dir\":[f\"{BASE_DIR}/tf_efficientnet_b6_ns_fold1.pth\",\n",
    "                   f\"{BASE_DIR}/tf_efficientnet_b6_ns_fold3.pth\",\n",
    "                   f\"{BASE_DIR}/tf_efficientnetv2_l_in21k_fold0.pth\",\n",
    "                   f\"{BASE_DIR}/tf_efficientnetv2_l_in21k_fold1.pth\",\n",
    "                   f\"{BASE_DIR}/tf_efficientnetv2_l_in21k_fold2.pth\",\n",
    "                   f\"{BASE_DIR}/tf_efficientnetv2_l_in21k_fold4.pth\",\n",
    "                   f\"{BASE_DIR}/eca_nfnet_l2_fold0.pth\",\n",
    "                   f\"{BASE_DIR}/eca_nfnet_l2_fold1.pth\",\n",
    "                   f\"{BASE_DIR}/eca_nfnet_l2_fold4.pth\",\n",
    "                   ],\n",
    "    \"model_name\": [\"tf_efficientnet_b6_ns\",\n",
    "                   \"tf_efficientnet_b6_ns\",\n",
    "                   \"tf_efficientnetv2_l_in21k\",\n",
    "                   \"tf_efficientnetv2_l_in21k\",\n",
    "                   \"tf_efficientnetv2_l_in21k\",\n",
    "                   \"tf_efficientnetv2_l_in21k\",\n",
    "                   \"eca_nfnet_l2\",\n",
    "                   \"eca_nfnet_l2\",\n",
    "                   \"eca_nfnet_l2\",\n",
    "                   ],\n",
    "    \"embedding_size\":[512, 512, 512, 512, 512, 512, 512, 512, 512],\n",
    "}\n",
    "\n",
    "train_embeds_list = []\n",
    "valid_embeds_list = []\n",
    "test_embeds_list = []\n",
    "train_labels_list = []\n",
    "valid_labels_list = []\n",
    "train_ids_list = []\n",
    "valid_ids_list = []\n",
    "test_ids_list = []\n",
    "\n",
    "for idx in range(len(model_weight_dict[\"weights_dir\"])):\n",
    "    weights_dir = model_weight_dict[\"weights_dir\"][idx]\n",
    "    model_name = model_weight_dict[\"model_name\"][idx]\n",
    "    embedding_size = model_weight_dict[\"embedding_size\"][idx] \n",
    "\n",
    "    model = HappyWhaleModel(model_name, embedding_size) \n",
    "    state = torch.load(weights_dir) \n",
    "\n",
    "    if CONFIG['gpu_parallel']:\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state.items():\n",
    "            k=k[7:]\n",
    "            new_state_dict[k]=v\n",
    "        model.load_state_dict(new_state_dict)\n",
    "    else:\n",
    "        model.load_state_dict(state)\n",
    "    model.to(CONFIG['device']) \n",
    "\n",
    "    train_embeds, train_labels, train_ids = get_embeddings(model, train_loader, CONFIG['device']) \n",
    "    valid_embeds, valid_labels, valid_ids = get_embeddings(model, valid_loader, CONFIG['device']) \n",
    "    test_embeds, _, test_ids = get_embeddings(model, test_loader, CONFIG['device']) \n",
    "\n",
    "    train_embeds_list.append(train_embeds)\n",
    "    valid_embeds_list.append(valid_embeds)\n",
    "    test_embeds_list.append(test_embeds)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    _ = gc.collect()\n",
    "\n",
    "train_embeds = np.concatenate(train_embeds_list,axis=1) \n",
    "valid_embeds = np.concatenate(valid_embeds_list,axis=1)\n",
    "test_embeds = np.concatenate(test_embeds_list,axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors \n",
    "neigh = NearestNeighbors(n_neighbors=CONFIG[\"KNN\"],metric='cosine')\n",
    "neigh.fit(train_embeds) \n",
    "valid_distances, valid_idxs = neigh.kneighbors(valid_embeds, CONFIG[\"KNN\"], return_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd9b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_allowed_labels = encoder.inverse_transform(train_labels) \n",
    "valid_allowed_labels = encoder.inverse_transform(valid_labels)\n",
    "\n",
    "train_allowed_labels_set = set(train_allowed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a341c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets_df = pd.DataFrame(np.stack([valid_ids, valid_allowed_labels], axis=1),columns=['image','target'])\n",
    "val_targets_df.loc[~val_targets_df.target.isin(train_allowed_labels_set),'target'] = 'new_individual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = []\n",
    "for i in tqdm(range(len(valid_ids))):\n",
    "    id_ = valid_ids[i]\n",
    "    targets = train_labels[valid_idxs[i]] \n",
    "    distances = valid_distances[i] \n",
    "    subset_preds = pd.DataFrame(np.stack([targets,distances],axis=1),columns=['target','distances'])\n",
    "    subset_preds['image'] = id_\n",
    "    valid_df.append(subset_preds)\n",
    "valid_df = pd.concat(valid_df).reset_index(drop=True) \n",
    "valid_df['confidence'] = 1-valid_df['distances']\n",
    "valid_df = valid_df.groupby(['image','target']).confidence.max().reset_index()\n",
    "valid_df = valid_df.sort_values('confidence',ascending=False).reset_index(drop=True) \n",
    "valid_df['target'] = encoder.inverse_transform(valid_df['target'].astype(\"int\").to_list()) \n",
    "valid_df.to_csv('val_neighbors.csv')\n",
    "valid_df.image.value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05879e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = ['938b7e931166', '5bf17305f073', '7593d2aee842', '7362d7a01d00','956562ff2888'] \n",
    "\n",
    "def get_predictions(test_df, threshold=0.2):\n",
    "    predictions = {} \n",
    "    for i, row in tqdm(test_df.iterrows()):\n",
    "        if row.image in predictions: \n",
    "            if len(predictions[row.image]) == 5: \n",
    "                continue\n",
    "            predictions[row.image].append(row.target) \n",
    "        elif row.confidence > threshold:\n",
    "            predictions[row.image] = [row.target, 'new_individual'] \n",
    "        else:\n",
    "            predictions[row.image] = ['new_individual', row.target] \n",
    "\n",
    "    for x in tqdm(predictions):\n",
    "        if len(predictions[x]) < 5:\n",
    "            remaining = [y for y in sample_list if y not in predictions] \n",
    "            predictions[x] = predictions[x] + remaining \n",
    "            predictions[x] = predictions[x][:5] \n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b542153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_per_image(label, predictions):\n",
    "    \"\"\"Computes the precision score of one image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : string\n",
    "            The true label of the image\n",
    "    predictions : list\n",
    "            A list of predicted elements (order does matter, 5 predictions allowed per image)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        return 1 / (predictions[:5].index(label) + 1)\n",
    "    except ValueError:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_th = 0  \n",
    "best_cv = 0\n",
    "for th in [0.1*x for x in range(11)]: \n",
    "    print(\"threshold:\", th)\n",
    "    all_preds = get_predictions(valid_df,threshold=th) \n",
    "    cv = 0\n",
    "    for i,row in val_targets_df.iterrows(): \n",
    "        target = row.target  \n",
    "        preds = all_preds[row.image]  \n",
    "        val_targets_df.loc[i,th] = map_per_image(target,preds) \n",
    "    cv = val_targets_df[th].mean() \n",
    "    print(f\"CV at threshold {th}: {cv}\")\n",
    "    if cv>best_cv:\n",
    "        best_th = th\n",
    "        best_cv = cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ffbe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T06:02:58.794305Z",
     "iopub.status.busy": "2022-02-23T06:02:58.793461Z",
     "iopub.status.idle": "2022-02-23T06:02:58.802367Z",
     "shell.execute_reply": "2022-02-23T06:02:58.801950Z",
     "shell.execute_reply.started": "2022-02-23T05:00:54.876684Z"
    },
    "papermill": {
     "duration": 0.336031,
     "end_time": "2022-02-23T06:02:58.802482",
     "exception": false,
     "start_time": "2022-02-23T06:02:58.466451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Best threshold\",best_th)\n",
    "print(\"Best cv\",best_cv) \n",
    "val_targets_df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e07716",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets_df['is_new_individual'] = val_targets_df.target=='new_individual' \n",
    "print(val_targets_df.is_new_individual.value_counts().to_dict())  \n",
    "val_scores = val_targets_df.groupby('is_new_individual').mean().T \n",
    "val_scores['adjusted_cv'] = val_scores[True]*0.15+val_scores[False]*0.85 \n",
    "best_threshold_adjusted = val_scores['adjusted_cv'].idxmax() \n",
    "print(\"best_threshold\",best_threshold_adjusted)\n",
    "val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1da02",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da6e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T06:08:30.698550Z",
     "iopub.status.busy": "2022-02-23T06:08:30.697347Z",
     "iopub.status.idle": "2022-02-23T06:08:30.738813Z",
     "shell.execute_reply": "2022-02-23T06:08:30.739504Z",
     "shell.execute_reply.started": "2022-02-23T05:00:55.684185Z"
    },
    "papermill": {
     "duration": 1.115278,
     "end_time": "2022-02-23T06:08:30.739714",
     "exception": false,
     "start_time": "2022-02-23T06:08:29.624436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_embeds = np.concatenate([train_embeds, valid_embeds]) \n",
    "all_labels = np.concatenate([train_labels, valid_labels]) \n",
    "print(all_embeds.shape, all_labels.shape)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=CONFIG[\"KNN\"],metric='cosine') \n",
    "neigh.fit(all_embeds) \n",
    "test_distances, test_idxs = neigh.kneighbors(test_embeds, CONFIG[\"KNN\"], return_distance=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(f'{ROOT_DIR}/sample_submission.csv', index_col='image') \n",
    "print(\"test_ids len:\",len(test_ids), \"sample_submission len:\",len(sample_submission))\n",
    "test_df = []\n",
    "for i in tqdm(range(len(test_ids))):  \n",
    "    id_ = test_ids[i]  \n",
    "    targets = all_labels[test_idxs[i]] \n",
    "    distances = test_distances[i] \n",
    "    subset_preds = pd.DataFrame(np.stack([targets,distances],axis=1),columns=['target','distances'])\n",
    "    subset_preds['image'] = id_\n",
    "    test_df.append(subset_preds)\n",
    "test_df = pd.concat(test_df).reset_index(drop=True)\n",
    "test_df['confidence'] = 1-test_df['distances']\n",
    "test_df = test_df.groupby(['image','target']).confidence.max().reset_index()\n",
    "test_df = test_df.sort_values('confidence',ascending=False).reset_index(drop=True) \n",
    "test_df['target'] = encoder.inverse_transform(test_df['target'].astype(\"int\").to_list()) \n",
    "test_df.to_csv('test_neighbors.csv')\n",
    "test_df.image.value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {} \n",
    "for i,row in tqdm(test_df.iterrows()):\n",
    "    if row.image in predictions: \n",
    "        if len(predictions[row.image])==5: \n",
    "            continue\n",
    "        predictions[row.image].append(row.target)\n",
    "    elif row.confidence>best_threshold_adjusted:\n",
    "        predictions[row.image] = [row.target,'new_individual'] \n",
    "    else:\n",
    "        predictions[row.image] = ['new_individual',row.target] \n",
    "        \n",
    "for x in tqdm(predictions):\n",
    "    if len(predictions[x])<5:\n",
    "        remaining = [y for y in sample_list if y not in predictions] \n",
    "        predictions[x] = predictions[x]+remaining \n",
    "        predictions[x] = predictions[x][:5] \n",
    "    predictions[x] = ' '.join(predictions[x])\n",
    "    \n",
    "predictions = pd.Series(predictions).reset_index()\n",
    "predictions.columns = ['image','predictions']\n",
    "predictions.to_csv('submission.csv',index=False)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a1056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5600.769643,
   "end_time": "2022-02-23T06:41:50.944700",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-23T05:08:30.175057",
   "version": "2.3.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
